'LDA Error Rate' = LDA.error.rate * 100,
'QDA Error Rate' = QDA.error.rate * 100,
'KNN-1 Error Rate' = KNN1.error.rate * 100,
'KNN-2 Error Rate' = KNN2.error.rate * 100,
'KNN-3 Error Rate' = KNN3.error.rate * 100,
'KNN-4 Error Rate' = KNN4.error.rate * 100)
t(Error.rate)
# Part a
AutoData <- Auto %>%
mutate(mpg01 = if_else(mpg > median(Auto$mpg), 1, 0))
# or another way to do the above
Auto$mpg01[Auto$mpg > median(Auto$mpg)] = 1
Auto$mpg01[Auto$mpg <= median(Auto$mpg)] = 0
# Part b
library(psych)
pairs.panels(AutoData[,],
smooth = TRUE,      # If TRUE, draws less smooths
scale = FALSE,      # If TRUE, scales the correlation text font
density = TRUE,     # If TRUE, adds density plots and histograms
ellipses = TRUE,    # If TRUE, draws ellipses
method = "pearson", # Correlation method (also "spearman" or "kendall")
pch = 21,           # pch symbol
lm = FALSE,         # If TRUE, plots linear fit rather than the LOESS (smoothed) fit
cor = TRUE,         # If TRUE, reports correlations
jiggle = FALSE,     # If TRUE, data points are jittered
factor = 2,         # Jittering factor
hist.col = 4,       # Histograms color
stars = TRUE,       # If TRUE, adds significance level with stars
ci = TRUE)          # If TRUE, adds confidence intervals
par(mfrow = c(2, 4))
plot(AutoData$horsepower, AutoData$mpg01)
plot(AutoData$weight, AutoData$mpg01)
plot(AutoData$displacement, AutoData$mpg01)
plot(AutoData$acceleration, AutoData$mpg01)
boxplot(AutoData$horsepower ~ AutoData$mpg01)
boxplot(AutoData$weight~ AutoData$mpg01)
boxplot(AutoData$displacement~ AutoData$mpg01)
boxplot(AutoData$acceleration~ AutoData$mpg01)
par(mfrow = c(1, 1))
# Part c - Create sample data
# create a vector with the size of number of records in the dataframe, 90% of values are True and 10% False randomly assigned, to be used as index
TrainIndex = sample(c(TRUE, FALSE), nrow(AutoData), replace = T, prob = c(0.9, 0.1))
TrainData = AutoData[TrainIndex,]       # just for the heck of it. subset command is used in the modeling
TestData = AutoData[!TrainIndex,]
# Part d
LDA.fit = lda(mpg01 ~ horsepower + weight + displacement + acceleration,
data = AutoData,
subset = TrainIndex)
LDA.fit
lda.pred = predict(LDA.fit, TestData)
lda.pred$class
table('LDA prediction' =lda.pred$class, observation = TestData$mpg01)
LDA.error.rate <- mean(TestData$mpg01 !=lda.pred$class)
# Part e
QDA.fit = qda(mpg01 ~ horsepower + weight + displacement + acceleration,
data = AutoData,
subset = TrainIndex)
QDA.fit
qda.pred = predict(QDA.fit,
newdata = TestData)
qda.pred$class
table('QDA prediction' =qda.pred$class, observation = TestData$mpg01)
QDA.error.rate <- mean(TestData$mpg01 !=qda.pred$class)
# Part f
logistic.fit = glm(mpg01 ~ horsepower + weight + displacement + acceleration,
data = AutoData,
subset = TrainIndex,
family = binomial)
summary(logistic.fit)
par(mfrow = c(2,2))
plot(logistic.fit)
par(mfrow = c(1,1))
logistic.prob = predict(logistic.fit,
newdata = TestData,
type = "response")
logistic.pred = rep(0, times = nrow(TestData))
logistic.pred[logistic.prob > 0.5] = 1
table('logistic prediction'= logistic.pred, 'Observation' = TestData$mpg01)
(logistic.error.rate = mean(logistic.pred != TestData$mpg01))
# Part g
SmarketTrain = Smarket[TrainIndex, 2:3]      # making the training set with only the first two variables
SmarketTest = Smarket[!TrainIndex, 2:3]      # making the test set with only the first two variables
SmarketTrainResponse = Smarket$Direction[TrainIndex]   # making the response vector for the training set
TrainAuto = AutoData[TrainIndex, 3:6]
TrainResponce = AutoData[TrainIndex, 10]
TestAuto = AutoData[!TrainIndex, 3:6]
KNN1.fit = knn(TrainAuto, TestAuto, TrainResponce, k = 1)
table('KNN prediction'= KNN1.fit, 'Observation' = TestData$mpg01)
(KNN1.error.rate = mean(KNN1.fit != TestData$mpg01))
KNN2.fit = knn(TrainAuto, TestAuto, TrainResponce, k = 2)
(KNN2.error.rate = mean(KNN2.fit != TestData$mpg01))
KNN3.fit = knn(TrainAuto, TestAuto, TrainResponce, k = 3)
(KNN3.error.rate = mean(KNN3.fit != TestData$mpg01))
KNN4.fit = knn(TrainAuto, TestAuto, TrainResponce, k = 4)
(KNN4.error.rate = mean(KNN4.fit != TestData$mpg01))
options(digits = 4)
Error.rate =data.frame('Logistic Error Rate' = logistic.error.rate * 100,
'LDA Error Rate' = LDA.error.rate * 100,
'QDA Error Rate' = QDA.error.rate * 100,
'KNN-1 Error Rate' = KNN1.error.rate * 100,
'KNN-2 Error Rate' = KNN2.error.rate * 100,
'KNN-3 Error Rate' = KNN3.error.rate * 100,
'KNN-4 Error Rate' = KNN4.error.rate * 100)
t(Error.rate)
AutoData <- Auto %>%
mutate(mpg01 = if_else(mpg > median(Auto$mpg), 1, 0))
# or another way to do the above
Auto$mpg01[Auto$mpg > median(Auto$mpg)] = 1
Auto$mpg01[Auto$mpg <= median(Auto$mpg)] = 0
# Part b
library(psych)
pairs.panels(AutoData[,],
smooth = TRUE,      # If TRUE, draws less smooths
scale = FALSE,      # If TRUE, scales the correlation text font
density = TRUE,     # If TRUE, adds density plots and histograms
ellipses = TRUE,    # If TRUE, draws ellipses
method = "pearson", # Correlation method (also "spearman" or "kendall")
pch = 21,           # pch symbol
lm = FALSE,         # If TRUE, plots linear fit rather than the LOESS (smoothed) fit
cor = TRUE,         # If TRUE, reports correlations
jiggle = FALSE,     # If TRUE, data points are jittered
factor = 2,         # Jittering factor
hist.col = 4,       # Histograms color
stars = TRUE,       # If TRUE, adds significance level with stars
ci = TRUE)          # If TRUE, adds confidence intervals
par(mfrow = c(2, 4))
plot(AutoData$horsepower, AutoData$mpg01)
plot(AutoData$weight, AutoData$mpg01)
plot(AutoData$displacement, AutoData$mpg01)
plot(AutoData$acceleration, AutoData$mpg01)
boxplot(AutoData$horsepower ~ AutoData$mpg01)
boxplot(AutoData$weight~ AutoData$mpg01)
boxplot(AutoData$displacement~ AutoData$mpg01)
boxplot(AutoData$acceleration~ AutoData$mpg01)
par(mfrow = c(1, 1))
# Part c - Create sample data
# create a vector with the size of number of records in the dataframe, 90% of values are True and 10% False randomly assigned, to be used as index
TrainIndex = sample(c(TRUE, FALSE), nrow(AutoData), replace = T, prob = c(0.9, 0.1))
TrainData = AutoData[TrainIndex,]       # just for the heck of it. subset command is used in the modeling
TestData = AutoData[!TrainIndex,]
# Part d
LDA.fit = lda(mpg01 ~ horsepower + weight + displacement + acceleration,
data = AutoData,
subset = TrainIndex)
LDA.fit
lda.pred = predict(LDA.fit, TestData)
lda.pred$class
table('LDA prediction' =lda.pred$class, observation = TestData$mpg01)
LDA.error.rate <- mean(TestData$mpg01 !=lda.pred$class)
# Part e
QDA.fit = qda(mpg01 ~ horsepower + weight + displacement + acceleration,
data = AutoData,
subset = TrainIndex)
QDA.fit
qda.pred = predict(QDA.fit,
newdata = TestData)
qda.pred$class
table('QDA prediction' =qda.pred$class, observation = TestData$mpg01)
QDA.error.rate <- mean(TestData$mpg01 !=qda.pred$class)
# Part f
logistic.fit = glm(mpg01 ~ horsepower + weight + displacement + acceleration,
data = AutoData,
subset = TrainIndex,
family = binomial)
summary(logistic.fit)
par(mfrow = c(2,2))
plot(logistic.fit)
par(mfrow = c(1,1))
logistic.prob = predict(logistic.fit,
newdata = TestData,
type = "response")
logistic.pred = rep(0, times = nrow(TestData))
logistic.pred[logistic.prob > 0.5] = 1
table('logistic prediction'= logistic.pred, 'Observation' = TestData$mpg01)
(logistic.error.rate = mean(logistic.pred != TestData$mpg01))
# Part g
SmarketTrain = Smarket[TrainIndex, 2:3]      # making the training set with only the first two variables
SmarketTest = Smarket[!TrainIndex, 2:3]      # making the test set with only the first two variables
SmarketTrainResponse = Smarket$Direction[TrainIndex]   # making the response vector for the training set
TrainAuto = AutoData[TrainIndex, 3:6]
TrainResponce = AutoData[TrainIndex, 10]
TestAuto = AutoData[!TrainIndex, 3:6]
KNN1.fit = knn(TrainAuto, TestAuto, TrainResponce, k = 1)
table('KNN prediction'= KNN1.fit, 'Observation' = TestData$mpg01)
(KNN1.error.rate = mean(KNN1.fit != TestData$mpg01))
KNN2.fit = knn(TrainAuto, TestAuto, TrainResponce, k = 2)
(KNN2.error.rate = mean(KNN2.fit != TestData$mpg01))
KNN3.fit = knn(TrainAuto, TestAuto, TrainResponce, k = 3)
(KNN3.error.rate = mean(KNN3.fit != TestData$mpg01))
KNN4.fit = knn(TrainAuto, TestAuto, TrainResponce, k = 4)
(KNN4.error.rate = mean(KNN4.fit != TestData$mpg01))
options(digits = 4)
Error.rate =data.frame('Logistic Error Rate' = logistic.error.rate * 100,
'LDA Error Rate' = LDA.error.rate * 100,
'QDA Error Rate' = QDA.error.rate * 100,
'KNN-1 Error Rate' = KNN1.error.rate * 100,
'KNN-2 Error Rate' = KNN2.error.rate * 100,
'KNN-3 Error Rate' = KNN3.error.rate * 100,
'KNN-4 Error Rate' = KNN4.error.rate * 100)
t(Error.rate)
######### Ch 4 - Ex 12 ########
# Part a
cubic.function = function() {
2^3
}
##### End ####
##@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
rm(list = ls()) ; dev.off() ; plot.new()
######### Ch 4 - Ex 12 ########
# Part a
cubic.function = function() {
2^3
}
cubic.function
cubic.function(2)
# write a function to avoid loading libraries every time
LoadLibs = function(){
libs = c("ISLR", "MASS", "tidyverse")
lapply(libs, require, character.only = T)
}
2^3
cubic.function
show(2^3)
cubic.function
cubic.function()
##### End ####
##@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
rm(list = ls()) ; dev.off() ; plot.new()
######### Ch 4 - Ex 12 ########
# Part a
Power = function() {
show(2^3)
}
Power()
# Part b
Power2 = function(a, b){
show(a^b)
}
Power2(2, 3)
Power2(4, 5)
# Part c
Power2(10, 3)
result = a^b
# Part d
Power3 = function(a, b){
result = a ^ b
return()
}
Power3(10, 3)
(Power3(10, 3))
# Part d
Power3 = function(a, b){
result = a ^ b
return(result)
}
Power3(10, 3)
lapply(c("MASS", "tidyverse", "ISLR", "car", "ggcorrplot", "psych", "class"),
require, character.only = T) # to load multiple packages at once
#### Sec 5.3.1 - The Validation Set Approach ####
# Using validation set approach to find the test error rate on linear models fit to the Auto data
AutoData
#### Sec 5.3.1 - The Validation Set Approach ####
# Using validation set approach to find the test error rate on linear models fit to the Auto data
data(Auto)
Auto
?data
data("starwars")
data(starwars)
starwars
Auto
head(Auto)
glimpse(Auto)
set.seed(1)
length(Auto)
nrow(Auto)
392/2
train = sample(392, 196)
train
?sample
train = sample(392, size = 196)
train = sample(392, size = 196)
set.seed(1)
train = sample(392, size = 196)
train
train = sample(392, size = 196)
train
train = sample(nrow(Auto), size = 0.5*nrow(Auto))
train
TrainIndex = sample(nrow(Auto), size = 0.5*nrow(Auto)) # generate random numbers between 1 and 392 for a total of 392/2
rm(train)
lm.fit1 = lm(mpg ~ horsepower,
data = Auto,
subset = TrainIndex)
summary(lm.fit1)
?lm
# Part c - Create sample data
# create a vector with the size of number of records in the dataframe, 90% of values are True and 10% False randomly assigned, to be used as index
TrainIndex1 = sample(c(TRUE, FALSE), nrow(AutoData), replace = T, prob = c(0.9, 0.1))
x = sample(c(TRUE, FALSE), nrow(Auto), replace = T, prob = c(0.9, 0.1))
x = sample(c(TRUE, FALSE), nrow(Auto), replace = T, prob = c(0.5, 0.5))
view(TrainIndex)
view(x)
TrainIndex = sample(392, 196) # generate random numbers between 1 and 392 for a total of 392/2
lm.fit1 = lm(mpg ~ horsepower,
data = Auto,
subset = TrainIndex)
summary(lm.fit1)
?predict
predict(lm.fit1, newdata = Auto)
Auto$mpg-predict(lm.fit1, newdata = Auto)
view(cbind(Auto$mpg, predict(lm.fit1, newdata = Auto), Auto$mpg-predict(lm.fit1, newdata = Auto)))
mean((Auto$mpg-predict(lm.fit1, newdata = Auto))[-train]^2)
mean((Auto$mpg-predict(lm.fit1, newdata = Auto))[-TrainIndex]^2)
view(cbind(Auto$mpg, predict(lm.fit1, newdata = Auto), Auto$mpg-predict(lm.fit1, newdata = Auto))[-TrainIndex])
view(cbind(Auto$mpg, predict(lm.fit1, newdata = Auto), Auto$mpg-predict(lm.fit1, newdata = Auto)))[-TrainIndex]
view(cbind(Auto$mpg, predict(lm.fit1, newdata = Auto), Auto$mpg-predict(lm.fit1, newdata = Auto)))
view(cbind(Auto$mpg, predict(lm.fit1, newdata = Auto), Auto$mpg-predict(lm.fit1, newdata = Auto)[-TrainIndex]))
Auto$mpg-predict(lm.fit1, newdata = Auto))[-TrainIndex]
(Auto$mpg-predict(lm.fit1, newdata = Auto))[-TrainIndex]
view((Auto$mpg-predict(lm.fit1, newdata = Auto))[-TrainIndex])
# Quadratic Linear Regression
lm.fit2 = lm(mpg ~ poly(horsepower, 2),
data = Auto,
subset = TrainIndex)
mean((Auto$mpg - predict(lm.fit2, newdata = Auto))[-TrainIndex]^2)
simpleMSE = mean((Auto$mpg - predict(lm.fit1, newdata = Auto))[-TrainIndex]^2)
QuadMSE = mean((Auto$mpg - predict(lm.fit2, newdata = Auto))[-TrainIndex]^2)
# Cubic Linear Regression
lm.fit3 = lm(mpg ~ poly(horsepower, 3),
data = Auto,
subset = TrainIndex)
CubicMSE = mean((Auto$mpg - predict(lm.fit3, newdata = Auto))[-TrainIndex]^2)
set.seed(1)
TrainIndex = sample(392, 196) # generate random numbers between 1 and 392 for a total of 392/2
# Simple Linear Regression
lm.fit1 = lm(mpg ~ horsepower,
data = Auto,
subset = TrainIndex)
# summary(lm.fit1)
# view(cbind(Auto$mpg, predict(lm.fit1, newdata = Auto), Auto$mpg-predict(lm.fit1, newdata = Auto)))
simpleMSE = mean((Auto$mpg - predict(lm.fit1, newdata = Auto))[-TrainIndex]^2)
# Quadratic Linear Regression
lm.fit2 = lm(mpg ~ poly(horsepower, 2),
data = Auto,
subset = TrainIndex)
QuadMSE = mean((Auto$mpg - predict(lm.fit2, newdata = Auto))[-TrainIndex]^2)
# Cubic Linear Regression
lm.fit3 = lm(mpg ~ poly(horsepower, 3),
data = Auto,
subset = TrainIndex)
CubicMSE = mean((Auto$mpg - predict(lm.fit3, newdata = Auto))[-TrainIndex]^2)
set.seed(1)
TrainIndex = sample(392, 196) # generate random numbers between 1 and 392 for a total of 392/2
# Simple Linear Regression
lm.fit1 = lm(mpg ~ horsepower,
data = Auto,
subset = TrainIndex)
# summary(lm.fit1)
# view(cbind(Auto$mpg, predict(lm.fit1, newdata = Auto), Auto$mpg-predict(lm.fit1, newdata = Auto)))
simpleMSE = mean((Auto$mpg - predict(lm.fit1, newdata = Auto))[-TrainIndex]^2)
# Quadratic Linear Regression
lm.fit2 = lm(mpg ~ poly(horsepower, 2),
data = Auto,
subset = TrainIndex)
QuadMSE = mean((Auto$mpg - predict(lm.fit2, newdata = Auto))[-TrainIndex]^2)
# Cubic Linear Regression
lm.fit3 = lm(mpg ~ poly(horsepower, 3),
data = Auto,
subset = TrainIndex)
CubicMSE = mean((Auto$mpg - predict(lm.fit3, newdata = Auto))[-TrainIndex]^2)
# set.seed(1)
TrainIndex = sample(392, 196) # generate random numbers between 1 and 392 for a total of 392/2
# Simple Linear Regression
lm.fit1 = lm(mpg ~ horsepower,
data = Auto,
subset = TrainIndex)
# summary(lm.fit1)
# view(cbind(Auto$mpg, predict(lm.fit1, newdata = Auto), Auto$mpg-predict(lm.fit1, newdata = Auto)))
simpleMSE = mean((Auto$mpg - predict(lm.fit1, newdata = Auto))[-TrainIndex]^2)
# Quadratic Linear Regression
lm.fit2 = lm(mpg ~ poly(horsepower, 2),
data = Auto,
subset = TrainIndex)
QuadMSE = mean((Auto$mpg - predict(lm.fit2, newdata = Auto))[-TrainIndex]^2)
# Cubic Linear Regression
lm.fit3 = lm(mpg ~ poly(horsepower, 3),
data = Auto,
subset = TrainIndex)
CubicMSE = mean((Auto$mpg - predict(lm.fit3, newdata = Auto))[-TrainIndex]^2)
View(starwars)
View(starwars)
# and for exporting data:
write.table(starwars, file="C:/Users/ma998/OneDrive/DATA/Starwars.csv",
row.names=FALSE,
col.names=True,
sep=",")
# and for exporting data:
write.table(starwars, file="C:/Users/ma998/OneDrive/DATA/Starwars.csv",
row.names=FALSE,
col.names=T,
sep=",")
# and for exporting data:
write.table(starwars, file="C:/Users/ma998/OneDrive/DATA/Starwars.csv",
row.names=FALSE,
col.names=T,
sep=",")
# and for exporting data:
write.table(starwars, file="C:/Users/ma998/OneDrive/DATA/Starwars.csv",
sep=",")
# and for exporting data:
write.table(starwars[,1:11], file="C:/Users/ma998/OneDrive/DATA/Starwars.csv",
row.names=FALSE,
col.names=T,
sep=",")
#### Sec 5.3.1 - The Validation Set Approach ####
# Using validation set approach to find the test error rate on linear models fit to the Auto data
data(Auto)
#### Sec 5.3.1 - The Validation Set Approach ####
# Using validation set approach to find the test error rate on linear models fit to the Auto data
data(Auto)
View(Auto)
glimpse(Auto)
lapply(c("MASS", "tidyverse", "ISLR", "car", "ggcorrplot", "psych", "class"),
require, character.only = T) # to load multiple packages at once
#### Sec 5.3.1 - The Validation Set Approach ####
# Using validation set approach to find the test error rate on linear models fit to the Auto data
data(Auto)
#### Sec 5.3.2 - The LOOCV Approach ####
# LOOCV estimate can be automatically computed for any generalized linear model using the glm() and cv.glm() functions
# in glm() if we don't specify the family option for it, it will do the same as a lm()
glm(mpg ~ horsepower, data = Auto) %>%
summary()
#### Sec 5.3.2 - The LOOCV Approach ####
# LOOCV estimate can be automatically computed for any generalized linear model using the glm() and cv.glm() functions
# in glm() if we don't specify the family option for it, it will do the same as a lm()
glm(mpg ~ horsepower, data = Auto) %>%
coef()
lm(mpg ~ horsepower, data = Auto) %>%
coef()
glm.fit = glm(mpg ~ horsepower, data = Auto)
cv.fit = cv.glm(Auto, glm.fit)
library(boot)
cv.fit = cv.glm(Auto, glm.fit)
cv.fit
?cv.glm
View(cv.fit)
cv.fit = cv.glm(data = Auto, glm.fit) # calculates the estimated K-fold cross-validation prediction error for generalized linear models.
glm.fit
rm(cv.fit)
LOOCV.error = cv.glm(data = Auto, glm.fit) # calculates the estimated K-fold cross-validation prediction error for generalized linear models (default k=n)
View(LOOCV.error)
LOOCV.error$delta
options(digits = 3)      # to show 3 decimal points in the outputs
LOOCV.error$delta
options(digits = 3)      # to show 3 decimal points in the outputs
LOOCV.error = cv.glm(data = Auto, glm.fit) # calculates the estimated K-fold cross-validation prediction error for generalized linear models (default k=n)
LOOCV.error$delta
options(digits = 4)      # to show 3 decimal points in the outputs
LOOCV.error$delta
options(digits = 5)      # to show 5 total digits in the output (which will be 3 decimals in this case)
LOOCV.error$delta
View(LOOCV.error)
View(LOOCV.error)
options(digits = 50)      # to show 5 total digits in the output (which will be 3 decimals in this case)
LOOCV.error$delta
options(digits = 10)      # to show 5 total digits in the output (which will be 3 decimals in this case)
LOOCV.error$delta
LOOCV.error = rep(0,5)
for (i in 1:5){
glm.fit = glm(mpg ~ poly(horsepower, i), data = Auto)
LOOCV.error[i] = cv.glm(data = Auto, glm.fit)$delta[1]
}
LOOCV.error
options(digits = 5)      # to show 5 total digits in the output (which will be 3 decimals in this case)
LOOCV.error = rep(0,5)
LOOCV.error
for (i in 1:5){
glm.fit = glm(mpg ~ poly(horsepower, i), data = Auto)
LOOCV.error[i] = cv.glm(data = Auto, glm.fit)$delta[1]
}
LOOCV.error
plot(LOOCV.error)
lineplot(LOOCV.error)
plot(LOOCV.error, type = "1")
LOOCV.error
type(LOOCV.error)
class(LOOCV.error)
plot(LOOCV.error)
plot(LOOCV.error, type = "o")
######################## Plotting ############################################################
plot(iris$Species)  # Categorical variable (becomes barchart by frequency)
plot(iris$Petal.Length)  # Quantitative variable (scatter plot by index of variable)
plot(x, type = "o", color = "red")
x <- c(1, 3, 6, 4, 9)
plot(x, type = "o", color = "red")
plot(LOOCV.error, type = "o")
set.seed(17)
kFold10.error = rep(0, 5)
kFold10.error[i] = cv.glm(data = Auto, glm.fit, K = 10)$delta[1]
for (i in 1:5) {
glm.fit = glm(mpg ~ poly(horsepower, i), data = Auto)
kFold10.error[i] = cv.glm(data = Auto, glm.fit, K = 10)$delta[1]
}
kFold10.error
plot(kFold10.error, type = "o")
for (i in 1:8) {
glm.fit = glm(mpg ~ poly(horsepower, i), data = Auto)
kFold10.error[i] = cv.glm(data = Auto, glm.fit, K = 10)$delta[1]
}
plot(kFold10.error, type = "o")
View(glm.fit)
cv.glm(data = Auto, glm.fit, K = 10)$delta
